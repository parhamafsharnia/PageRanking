{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Parham Afsharnia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read dataset cran.all.1400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/cran.all.1400'\n",
    "def readFile(path):\n",
    "    '''\n",
    "    This function returns raw text in given directory'''\n",
    "#     import nltk.Text\n",
    "    with open(path, 'r') as text:\n",
    "        return text.read()\n",
    "raw = readFile(path=path)\n",
    "\n",
    "def makeDatabase(raw_text):\n",
    "    '''this function get all cran.1400 raw text\n",
    "    and return list of subsets - [[doc1],[doc2],[doc3],...] - \n",
    "    raw text of subsets'''\n",
    "    data_base = []\n",
    "    for i in range (1,1401):\n",
    "        s = raw.find('.I ' + str(i))\n",
    "        e = raw.find('.I ' + str(i + 1))\n",
    "        data_base.append(raw[s:e])\n",
    "    return data_base\n",
    "raw_db = makeDatabase(raw_text=raw)\n",
    "# raw_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_page(page_id=None,db=None):\n",
    "    if page_id is None or db is None:\n",
    "        return 'Error'\n",
    "    return db[page_id - 1:page_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text,isalpha=False):\n",
    "    '''this function get str type data as input and return \n",
    "    list of tokens - with isalpha parameter, it can remove numbers if True - \n",
    "    also it eliminates all kinds of whitespace (\\r,\\n,\\t ...) and all non-alphabetical character (\"'()<>. ...)  '''\n",
    "    import re \n",
    "    if isalpha:\n",
    "        return [w for w in re.findall(r'\\w+|\\s+', text) if w.isalpha()]\n",
    "    return [w for w in re.findall(r'\\w+', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessor(raw_db):\n",
    "    '''this function will get raw_db list type dataset and return list type tokens dataset -\n",
    "    ([[doc1 tokens],[doc2 tokens],[doc3 tokens],...)'''\n",
    "    token_db = []\n",
    "    for doc in range (len(raw_db)):\n",
    "        token_db.append(tokenizer(raw_db[doc]))\n",
    "    return token_db\n",
    "token_db = preProcessor(raw_db=raw_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. a, b, c, d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of occurence for all the words\n",
    "def freqDist(token_dataset):\n",
    "    '''input: tokens dataset\n",
    "    output: nltk.FreqDist() '''\n",
    "    import nltk\n",
    "    return nltk.FreqDist(token_dataset)\n",
    "\n",
    "def tokenCollector(token_dataset):\n",
    "    '''input: token dataset\n",
    "    output: cran.1400 collection tokenized list type '''\n",
    "    tokens = []\n",
    "    for token_set in token_dataset:\n",
    "        tokens.extend(token_set)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 20204, 'of': 14032, 'and': 7116, 'a': 6835, 'in': 5034, 'to': 4725, 'is': 4118, 'for': 3714, 'with': 2444, 'are': 2431, ...})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenCollector(token_dataset=token_db)\n",
    "frequency = freqDist(token_dataset=tokens)\n",
    "# frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "def vocabularyGetter(token_list):\n",
    "    '''input: token list\n",
    "    output: vocabulary list - (^a-b,A-B)'''\n",
    "    return [w for w in set(token_list) if w.isalpha() and len(w)>1]\n",
    "\n",
    "# print(vocabulary(token_db[0]))\n",
    "\n",
    "def sizeGetter(vocabulary_list):\n",
    "    '''input: vocabulary list\n",
    "    output: vocabulary size'''\n",
    "    return len(vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8149"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vocabularyGetter(token_list=tokens)\n",
    "vocab_size = sizeGetter(vocabulary_list=vocab)\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "def mostFreq(freqdist,size=10):\n",
    "    '''input: freqdist list\n",
    "    output: most common words'''\n",
    "    return freqdist.most_common(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common word(s):\n",
      " [('the', 20204), ('of', 14032), ('and', 7116), ('a', 6835), ('in', 5034), ('to', 4725), ('is', 4118), ('for', 3714), ('with', 2444), ('are', 2431)]\n"
     ]
    }
   ],
   "source": [
    "most_freq = mostFreq(freqdist=frequency)\n",
    "print(len(most_freq),'most common word(s):\\n', most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "def meaningful_stopword(most_common):\n",
    "    '''input: most common list of mostFreq function's output\n",
    "       output: meaningful word list, stopword list'''\n",
    "    from nltk.corpus import stopwords\n",
    "    meaningful = []\n",
    "    stopword = []\n",
    "    for word in most_common:\n",
    "        if word[0] in stopwords.words('english') or len(word[0]) == 1:\n",
    "            stopword.append(word)\n",
    "        else:\n",
    "            meaningful.append(word)\n",
    "    return meaningful,stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaningful:\n",
      " []\n",
      "stopword:\n",
      " [('the', 20204), ('of', 14032), ('and', 7116), ('a', 6835), ('in', 5034), ('to', 4725), ('is', 4118), ('for', 3714), ('with', 2444), ('are', 2431)]\n"
     ]
    }
   ],
   "source": [
    "meaningful,stopword=meaningful_stopword(most_common=most_freq)\n",
    "print('meaningful:\\n', meaningful)\n",
    "print('stopword:\\n', stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d\n",
    "def minHalf(freqdist):\n",
    "    '''return words that are half of the tokens dataset !!!\n",
    "    input: freqdist list of dataset'''\n",
    "    size=0\n",
    "    for word in freqdist.items():\n",
    "        size = size + word[1]\n",
    "#     print(size)\n",
    "    half_size = size // 2\n",
    "#     print(half_size)\n",
    "    hsum = 0\n",
    "    counter = 0\n",
    "    for i in freqdist.most_common():\n",
    "        if hsum < half_size:\n",
    "            hsum = hsum + i[1]\n",
    "            counter += 1\n",
    "    #         print(counter,i[1])\n",
    "    if hsum > half_size:\n",
    "        counter -= 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum number of words: 92\n",
      "# this number represent the most commen words that contain half of the text \n"
     ]
    }
   ],
   "source": [
    "print('minimum number of words:',minHalf(freqdist=frequency))\n",
    "print('# this number represent the most commen words that contain half of the text ')\n",
    "# this number represent the most commen words that contain half of the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordRemover(token_list):\n",
    "    '''input: list of words\n",
    "    output: list of words except stopwords'''\n",
    "    from nltk.corpus import stopwords\n",
    "    filtered_words = [word for word in token_list if  word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "def porter_stemmer(word_list):\n",
    "    '''nltk porter stemmer\n",
    "    input: word list\n",
    "    output: stemmed list'''\n",
    "    porterstemmer = []\n",
    "    # import algorithms from nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    # PS stands for PortersStemmer's Class Object\n",
    "    PS = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        porterstemmer.append(PS.stem(word))\n",
    "    return porterstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " '1',\n",
       " 't',\n",
       " 'experiment',\n",
       " 'investig',\n",
       " 'aerodynam',\n",
       " 'wing',\n",
       " 'slipstream',\n",
       " 'a',\n",
       " 'brenckman',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '25',\n",
       " '1958',\n",
       " '324',\n",
       " 'w',\n",
       " 'experiment',\n",
       " 'investig',\n",
       " 'aerodynam',\n",
       " 'wing',\n",
       " 'slipstream',\n",
       " 'experiment',\n",
       " 'studi',\n",
       " 'wing',\n",
       " 'propel',\n",
       " 'slipstream',\n",
       " 'made',\n",
       " 'order',\n",
       " 'determin',\n",
       " 'spanwis',\n",
       " 'distribut',\n",
       " 'lift',\n",
       " 'increas',\n",
       " 'due',\n",
       " 'slipstream',\n",
       " 'differ',\n",
       " 'angl',\n",
       " 'attack',\n",
       " 'wing',\n",
       " 'differ',\n",
       " 'free',\n",
       " 'stream',\n",
       " 'slipstream',\n",
       " 'veloc',\n",
       " 'ratio',\n",
       " 'result',\n",
       " 'intend',\n",
       " 'part',\n",
       " 'evalu',\n",
       " 'basi',\n",
       " 'differ',\n",
       " 'theoret',\n",
       " 'treatment',\n",
       " 'problem',\n",
       " 'compar',\n",
       " 'span',\n",
       " 'load',\n",
       " 'curv',\n",
       " 'togeth',\n",
       " 'support',\n",
       " 'evid',\n",
       " 'show',\n",
       " 'substanti',\n",
       " 'part',\n",
       " 'lift',\n",
       " 'increment',\n",
       " 'produc',\n",
       " 'slipstream',\n",
       " 'due',\n",
       " 'destal',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'control',\n",
       " 'effect',\n",
       " 'integr',\n",
       " 'remain',\n",
       " 'lift',\n",
       " 'increment',\n",
       " 'subtract',\n",
       " 'destal',\n",
       " 'lift',\n",
       " 'found',\n",
       " 'agre',\n",
       " 'well',\n",
       " 'potenti',\n",
       " 'flow',\n",
       " 'theori',\n",
       " 'empir',\n",
       " 'evalu',\n",
       " 'destal',\n",
       " 'effect',\n",
       " 'made',\n",
       " 'specif',\n",
       " 'configur',\n",
       " 'experi',\n",
       " 'i',\n",
       " '2',\n",
       " 't',\n",
       " 'simpl',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'incompress',\n",
       " 'fluid',\n",
       " 'small',\n",
       " 'viscos',\n",
       " 'a',\n",
       " 'ting',\n",
       " 'yili',\n",
       " 'b',\n",
       " 'depart',\n",
       " 'aeronaut',\n",
       " 'engin',\n",
       " 'renssela',\n",
       " 'polytechn',\n",
       " 'institut',\n",
       " 'troy',\n",
       " 'n',\n",
       " 'w',\n",
       " 'simpl',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'incompress',\n",
       " 'fluid',\n",
       " 'small',\n",
       " 'viscos',\n",
       " 'studi',\n",
       " 'high',\n",
       " 'speed',\n",
       " 'viscou',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'bodi',\n",
       " 'usual',\n",
       " 'necessari',\n",
       " 'consid',\n",
       " 'curv',\n",
       " 'shock',\n",
       " 'wave',\n",
       " 'emit',\n",
       " 'nose',\n",
       " 'lead',\n",
       " 'edg',\n",
       " 'bodi',\n",
       " 'consequ',\n",
       " 'exist',\n",
       " 'inviscid',\n",
       " 'rotat',\n",
       " 'flow',\n",
       " 'region',\n",
       " 'shock',\n",
       " 'wave',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'situat',\n",
       " 'aris',\n",
       " 'instanc',\n",
       " 'studi',\n",
       " 'hyperson',\n",
       " 'viscou',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'situat',\n",
       " 'somewhat',\n",
       " 'differ',\n",
       " 'prandtl',\n",
       " 'classic',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'problem',\n",
       " 'prandtl',\n",
       " 'origin',\n",
       " 'problem',\n",
       " 'inviscid',\n",
       " 'free',\n",
       " 'stream',\n",
       " 'outsid',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'irrot',\n",
       " 'hyperson',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'problem',\n",
       " 'inviscid',\n",
       " 'free',\n",
       " 'stream',\n",
       " 'must',\n",
       " 'consid',\n",
       " 'rotat',\n",
       " 'possibl',\n",
       " 'effect',\n",
       " 'vortic',\n",
       " 'recent',\n",
       " 'discuss',\n",
       " 'ferri',\n",
       " 'libbi',\n",
       " 'present',\n",
       " 'paper',\n",
       " 'simpl',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'fluid',\n",
       " 'small',\n",
       " 'viscos',\n",
       " 'investig',\n",
       " 'shown',\n",
       " 'problem',\n",
       " 'treat',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'approxim',\n",
       " 'novel',\n",
       " 'featur',\n",
       " 'free',\n",
       " 'stream',\n",
       " 'constant',\n",
       " 'vortic',\n",
       " 'discuss',\n",
       " 'restrict',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'incompress',\n",
       " 'steadi',\n",
       " 'flow',\n",
       " 'i',\n",
       " '3',\n",
       " 't',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'simpl',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'a',\n",
       " 'b',\n",
       " 'glauert',\n",
       " 'b',\n",
       " 'depart',\n",
       " 'mathemat',\n",
       " 'univers',\n",
       " 'manchest',\n",
       " 'manchest',\n",
       " 'england',\n",
       " 'w',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'simpl',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'past',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'equat',\n",
       " 'present',\n",
       " 'steadi',\n",
       " 'incompress',\n",
       " 'flow',\n",
       " 'pressur',\n",
       " 'gradient',\n",
       " 'i',\n",
       " '4',\n",
       " 't',\n",
       " 'approxim',\n",
       " 'solut',\n",
       " 'incompress',\n",
       " 'laminar',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'equat',\n",
       " 'plate',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'a',\n",
       " 'yen',\n",
       " 'k',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '22',\n",
       " '1955',\n",
       " '728',\n",
       " 'w',\n",
       " 'approxim',\n",
       " 'solut',\n",
       " 'incompress',\n",
       " 'laminar',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'equat',\n",
       " 'plate',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'steadi',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'problem',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'shear',\n",
       " 'flow',\n",
       " 'incompress',\n",
       " 'fluid',\n",
       " 'consid',\n",
       " 'solut',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'thick',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'veloc',\n",
       " 'distribut',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'obtain',\n",
       " 'karman',\n",
       " 'pohlhausen',\n",
       " 'techniqu',\n",
       " 'comparison',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'uniform',\n",
       " 'flow',\n",
       " 'also',\n",
       " 'made',\n",
       " 'show',\n",
       " 'effect',\n",
       " 'vortic',\n",
       " 'i',\n",
       " '5',\n",
       " 't',\n",
       " 'one',\n",
       " 'dimension',\n",
       " 'transient',\n",
       " 'heat',\n",
       " 'conduct',\n",
       " 'doubl',\n",
       " 'layer',\n",
       " 'slab',\n",
       " 'subject',\n",
       " 'linear',\n",
       " 'heat',\n",
       " 'input',\n",
       " 'small',\n",
       " 'time',\n",
       " 'intern',\n",
       " 'a',\n",
       " 'wasserman',\n",
       " 'b',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '24',\n",
       " '1957',\n",
       " '924',\n",
       " 'w',\n",
       " 'one',\n",
       " 'dimension',\n",
       " 'transient',\n",
       " 'heat',\n",
       " 'conduct',\n",
       " 'doubl',\n",
       " 'layer',\n",
       " 'slab',\n",
       " 'subject',\n",
       " 'linear',\n",
       " 'heat',\n",
       " 'input',\n",
       " 'small',\n",
       " 'time',\n",
       " 'intern',\n",
       " 'analyt',\n",
       " 'solut',\n",
       " 'present',\n",
       " 'transient',\n",
       " 'heat',\n",
       " 'conduct',\n",
       " 'composit',\n",
       " 'slab',\n",
       " 'expos',\n",
       " 'one',\n",
       " 'surfac',\n",
       " 'triangular',\n",
       " 'heat',\n",
       " 'rate',\n",
       " 'type',\n",
       " 'heat',\n",
       " 'rate',\n",
       " 'may',\n",
       " 'occur',\n",
       " 'exampl',\n",
       " 'aerodynam',\n",
       " 'heat',\n",
       " 'i',\n",
       " '6',\n",
       " 't',\n",
       " 'one',\n",
       " 'dimension',\n",
       " 'transient',\n",
       " 'heat',\n",
       " 'flow',\n",
       " 'multilay',\n",
       " 'slab',\n",
       " 'a',\n",
       " 'campbel',\n",
       " 'w',\n",
       " 'f',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '25',\n",
       " '1958',\n",
       " '340',\n",
       " 'w',\n",
       " 'one',\n",
       " 'dimension',\n",
       " 'transient',\n",
       " 'heat',\n",
       " 'flow',\n",
       " 'multilay',\n",
       " 'slab',\n",
       " 'recent',\n",
       " 'contribut',\n",
       " 'reader',\n",
       " 'forum',\n",
       " 'wassermann',\n",
       " 'gave',\n",
       " 'analyt',\n",
       " 'solut',\n",
       " 'temperatur',\n",
       " 'doubl',\n",
       " 'layer',\n",
       " 'slab',\n",
       " 'triangular',\n",
       " 'heat',\n",
       " 'rate',\n",
       " 'input',\n",
       " 'one',\n",
       " 'face',\n",
       " 'insul',\n",
       " 'thermal',\n",
       " 'resist',\n",
       " 'interfac',\n",
       " 'solut',\n",
       " 'three',\n",
       " 'particular',\n",
       " 'case',\n",
       " 'propos',\n",
       " 'give',\n",
       " 'gener',\n",
       " 'solut',\n",
       " 'problem',\n",
       " 'indic',\n",
       " 'briefli',\n",
       " 'obtain',\n",
       " 'use',\n",
       " 'method',\n",
       " 'refer',\n",
       " '2',\n",
       " 'point',\n",
       " 'solut',\n",
       " 'given',\n",
       " 'wassermann',\n",
       " 'incomplet',\n",
       " 'time',\n",
       " 'longer',\n",
       " 'durat',\n",
       " 'heat',\n",
       " 'input',\n",
       " 'i',\n",
       " '7',\n",
       " 't',\n",
       " 'effect',\n",
       " 'control',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'transit',\n",
       " 'superson',\n",
       " 'speed',\n",
       " 'a',\n",
       " 'van',\n",
       " 'driest',\n",
       " 'e',\n",
       " 'r',\n",
       " 'mccauley',\n",
       " 'w',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '27',\n",
       " '1960',\n",
       " '261',\n",
       " 'w',\n",
       " 'effect',\n",
       " 'control',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'transit',\n",
       " 'superson',\n",
       " 'speed',\n",
       " 'experi',\n",
       " 'perform',\n",
       " '12',\n",
       " 'superson',\n",
       " 'wind',\n",
       " 'tunnel',\n",
       " 'jet',\n",
       " 'propuls',\n",
       " 'laboratori',\n",
       " 'california',\n",
       " 'institut',\n",
       " 'technolog',\n",
       " 'investig',\n",
       " 'effect',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'sphere',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'transit',\n",
       " 'tain',\n",
       " 'local',\n",
       " 'mach',\n",
       " 'number',\n",
       " '1',\n",
       " '90',\n",
       " '2',\n",
       " '71',\n",
       " '3',\n",
       " '67',\n",
       " 'vari',\n",
       " 'trip',\n",
       " 'size',\n",
       " 'posit',\n",
       " 'space',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'per',\n",
       " 'inch',\n",
       " 'result',\n",
       " 'indic',\n",
       " '1',\n",
       " 'transit',\n",
       " 'laminar',\n",
       " 'turbul',\n",
       " 'flow',\n",
       " 'induc',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'begin',\n",
       " 'doubl',\n",
       " 'row',\n",
       " 'spiral',\n",
       " 'vortic',\n",
       " 'trail',\n",
       " 'element',\n",
       " 'contamin',\n",
       " 'break',\n",
       " 'surround',\n",
       " 'field',\n",
       " 'vortic',\n",
       " '2',\n",
       " 'transit',\n",
       " 'appear',\n",
       " 'rather',\n",
       " 'suddenli',\n",
       " 'becom',\n",
       " 'violent',\n",
       " 'increas',\n",
       " 'rough',\n",
       " 'height',\n",
       " 'rel',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'thick',\n",
       " '3',\n",
       " 'breakdown',\n",
       " 'vortic',\n",
       " 'field',\n",
       " 'strength',\n",
       " 'spiral',\n",
       " 'vortic',\n",
       " 'may',\n",
       " 'still',\n",
       " 'persist',\n",
       " 'sublay',\n",
       " 'ensu',\n",
       " 'turbul',\n",
       " 'flow',\n",
       " '4',\n",
       " 'later',\n",
       " 'space',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'littl',\n",
       " 'effect',\n",
       " 'upon',\n",
       " 'initi',\n",
       " 'breakdown',\n",
       " 'contamin',\n",
       " 'laminar',\n",
       " 'flow',\n",
       " '5',\n",
       " 'trip',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'u',\n",
       " 'v',\n",
       " 'veloc',\n",
       " 'kinemat',\n",
       " 'viscos',\n",
       " 'outer',\n",
       " 'edg',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'k',\n",
       " 'rough',\n",
       " 'height',\n",
       " 'transit',\n",
       " 'occur',\n",
       " 'rough',\n",
       " 'posit',\n",
       " 'vari',\n",
       " 'posit',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'one',\n",
       " 'fourth',\n",
       " 'power',\n",
       " 'viz',\n",
       " 'x',\n",
       " 'trip',\n",
       " 'posit',\n",
       " 'i',\n",
       " '8',\n",
       " 't',\n",
       " 'measur',\n",
       " 'effect',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'transit',\n",
       " 'a',\n",
       " 'klebanoff',\n",
       " 'p',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '22',\n",
       " '1955',\n",
       " '803',\n",
       " 'w',\n",
       " 'measur',\n",
       " 'effect',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'transit',\n",
       " 'studi',\n",
       " 'effect',\n",
       " 'rough',\n",
       " 'transit',\n",
       " 'h',\n",
       " 'l',\n",
       " 'dryden',\n",
       " 'found',\n",
       " 'basi',\n",
       " 'avail',\n",
       " 'data',\n",
       " 'effect',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'trip',\n",
       " 'wire',\n",
       " 'could',\n",
       " 'repres',\n",
       " 'reason',\n",
       " 'well',\n",
       " 'term',\n",
       " 'function',\n",
       " 'relat',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'transit',\n",
       " 'base',\n",
       " 'distanc',\n",
       " 'lead',\n",
       " 'edg',\n",
       " 'height',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'displac',\n",
       " 'thick',\n",
       " 'posit',\n",
       " 'element',\n",
       " 'suggest',\n",
       " 'addit',\n",
       " 'data',\n",
       " 'obtain',\n",
       " 'primarili',\n",
       " 'extend',\n",
       " 'rang',\n",
       " 'higher',\n",
       " 'valu',\n",
       " 'cours',\n",
       " 'investig',\n",
       " 'transit',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'conduct',\n",
       " 'nation',\n",
       " 'bureau',\n",
       " 'standard',\n",
       " 'result',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'obtain',\n",
       " 'appear',\n",
       " 'desir',\n",
       " 'see',\n",
       " 'whether',\n",
       " 'row',\n",
       " 'three',\n",
       " 'dimension',\n",
       " 'rough',\n",
       " 'element',\n",
       " 'would',\n",
       " 'behav',\n",
       " 'way',\n",
       " 'i',\n",
       " '9',\n",
       " 't',\n",
       " 'transit',\n",
       " 'studi',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'measur',\n",
       " 'insul',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'mach',\n",
       " 'number',\n",
       " '5',\n",
       " '8',\n",
       " 'a',\n",
       " 'korkegi',\n",
       " 'r',\n",
       " 'h',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ae',\n",
       " 'sc',\n",
       " '23',\n",
       " '1956',\n",
       " '97',\n",
       " 'w',\n",
       " 'transit',\n",
       " 'studi',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'measur',\n",
       " 'insul',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'mach',\n",
       " 'number',\n",
       " '5',\n",
       " '8',\n",
       " 'investig',\n",
       " 'transit',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'insul',\n",
       " 'flat',\n",
       " 'plate',\n",
       " '5',\n",
       " '26',\n",
       " 'made',\n",
       " 'galcit',\n",
       " '5',\n",
       " '5',\n",
       " 'hyperson',\n",
       " 'wind',\n",
       " 'tunnel',\n",
       " 'nomin',\n",
       " 'mach',\n",
       " 'number',\n",
       " '5',\n",
       " '8',\n",
       " 'phosphoresc',\n",
       " 'lacquer',\n",
       " 'techniqu',\n",
       " 'use',\n",
       " 'transit',\n",
       " 'detect',\n",
       " 'found',\n",
       " 'good',\n",
       " 'agreement',\n",
       " 'total',\n",
       " 'head',\n",
       " 'rake',\n",
       " 'measur',\n",
       " 'along',\n",
       " 'plate',\n",
       " 'surfac',\n",
       " 'pitot',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'survey',\n",
       " 'found',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'laminar',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'least',\n",
       " '5',\n",
       " 'x',\n",
       " '10',\n",
       " 'transvers',\n",
       " 'contamin',\n",
       " 'caus',\n",
       " 'turbul',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'tunnel',\n",
       " 'sidewal',\n",
       " 'origin',\n",
       " 'far',\n",
       " 'downstream',\n",
       " 'flat',\n",
       " 'plate',\n",
       " 'lead',\n",
       " 'edg',\n",
       " 'reynold',\n",
       " 'number',\n",
       " '1',\n",
       " '5',\n",
       " '2',\n",
       " 'x',\n",
       " '10',\n",
       " 'spread',\n",
       " 'uniform',\n",
       " 'angl',\n",
       " '5',\n",
       " 'compar',\n",
       " '9',\n",
       " 'degre',\n",
       " 'low',\n",
       " 'speed',\n",
       " 'flow',\n",
       " 'effect',\n",
       " 'two',\n",
       " 'dimension',\n",
       " 'local',\n",
       " 'disturb',\n",
       " 'investig',\n",
       " 'techniqu',\n",
       " 'air',\n",
       " 'inject',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'mean',\n",
       " 'hasten',\n",
       " 'transit',\n",
       " 'extens',\n",
       " 'use',\n",
       " 'although',\n",
       " 'onset',\n",
       " 'transit',\n",
       " 'occur',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'low',\n",
       " '10',\n",
       " 'fulli',\n",
       " 'develop',\n",
       " 'turbul',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'obtain',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'much',\n",
       " '2',\n",
       " 'x',\n",
       " '10',\n",
       " 'regardless',\n",
       " 'amount',\n",
       " 'air',\n",
       " 'inject',\n",
       " 'qualit',\n",
       " 'discuss',\n",
       " 'result',\n",
       " 'given',\n",
       " 'emphasi',\n",
       " 'possibl',\n",
       " 'greater',\n",
       " 'stabil',\n",
       " 'laminar',\n",
       " 'boundari',\n",
       " 'layer',\n",
       " 'hyperson',\n",
       " 'flow',\n",
       " 'lower',\n",
       " 'speed',\n",
       " 'direct',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'measur',\n",
       " 'made',\n",
       " 'mean',\n",
       " 'float',\n",
       " 'element',\n",
       " 'techniqu',\n",
       " 'rang',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'verifi',\n",
       " 'laminar',\n",
       " 'complet',\n",
       " 'rang',\n",
       " 'air',\n",
       " 'inject',\n",
       " 'turbul',\n",
       " 'shear',\n",
       " 'obtain',\n",
       " 'reynold',\n",
       " 'number',\n",
       " 'greater',\n",
       " '2',\n",
       " 'x',\n",
       " '10',\n",
       " 'valu',\n",
       " 'good',\n",
       " 'agreement',\n",
       " 'earlier',\n",
       " 'result',\n",
       " 'investig',\n",
       " 'turbul',\n",
       " 'skin',\n",
       " 'friction',\n",
       " 'coeffici',\n",
       " 'found',\n",
       " 'approxim',\n",
       " '0',\n",
       " '40',\n",
       " 'incompress',\n",
       " 'flow',\n",
       " ...]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords = stopWordRemover(token_list=tokens)\n",
    "# tokens_without_stopwords\n",
    "stemm_list = porter_stemmer(word_list=tokens_without_stopwords)\n",
    "# stemm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5355"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "vocab_without_stopword = vocabularyGetter(token_list=stemm_list)\n",
    "vocab_without_stopword_size = sizeGetter(vocabulary_list=vocab_without_stopword)\n",
    "vocab_without_stopword_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common word(s):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flow', 2184),\n",
       " ('W', 1402),\n",
       " ('A', 1401),\n",
       " ('B', 1401),\n",
       " ('I', 1400),\n",
       " ('T', 1400),\n",
       " ('boundary', 1373),\n",
       " ('pressure', 1331),\n",
       " ('layer', 1192),\n",
       " ('number', 1033)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b\n",
    "frequency_without_stopword = freqDist(token_dataset=tokens_without_stopwords)\n",
    "most_freq_without_stopword = mostFreq(freqdist=frequency_without_stopword)\n",
    "print(len(most_freq_without_stopword), 'most common word(s):\\n')\n",
    "most_freq_without_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaningful:\n",
      " [('flow', 2184), ('boundary', 1373), ('pressure', 1331), ('layer', 1192), ('number', 1033)]\n",
      "meaningless:\n",
      " [('W', 1402), ('A', 1401), ('B', 1401), ('I', 1400), ('T', 1400)]\n"
     ]
    }
   ],
   "source": [
    "# c \n",
    "meaningful,meaningless=meaningful_stopword(most_common=most_freq_without_stopword)\n",
    "print('meaningful:\\n', meaningful)\n",
    "print('meaningless:\\n', meaningless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum number of words: 247\n",
      "# this number represent the most commen words that contain half of the text \n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "print('minimum number of words:',minHalf(freqdist=frequency_without_stopword))\n",
    "print('# this number represent the most commen words that contain half of the text ')\n",
    "# this number represent the most commen words that contain half of the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapsLaw(token_size,k=44,beta=0.49):\n",
    "    '''heaps law function\n",
    "    input: token size\n",
    "    output: prediction of vocabulary size '''\n",
    "    import math\n",
    "#     vocab = vocabulary(text)\n",
    "#     T = vocabulary_size(vocab)\n",
    "    T = token_size\n",
    "    M = k * T ** beta\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27286.509153967454"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heapsLaw(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53821.69543820025"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heapsLaw(2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term,document):\n",
    "    '''this function is tf algorithm's imp\n",
    "    input: term,document\n",
    "    output:tf measure'''\n",
    "    import math\n",
    "    f = document.count(term)\n",
    "    if f > 0:\n",
    "        return 1 + math.log10(f)\n",
    "    if f == 0:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term,token_db):\n",
    "    '''this function is idf algorithm's imp\n",
    "    input:term,token dataset\n",
    "    output: idf measure'''\n",
    "    import math\n",
    "    counter = 0\n",
    "    for i in range (len(token_db)):\n",
    "        if term in token_db[i]:\n",
    "            counter += 1\n",
    "    if counter == 0:\n",
    "        return counter\n",
    "    return math.log10(len(token_db) / counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_weight(tf=0, idf=0):\n",
    "    '''this function weighting a term in database\n",
    "    input:tf and idf measures\n",
    "    output:tf_idf weighing measure'''\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(query,token_db):\n",
    "    '''this function ranks or scores query tokens based on tf_id weighting\n",
    "    this is vectoral index based on query input\n",
    "    input:query (can be sentence or a single word),token_db\n",
    "    out_put:ranking list in order to get best relevant documents'''\n",
    "    ranking_list = {}\n",
    "    for i in range(len(token_db)):\n",
    "        ranking_list[i] = 0\n",
    "    for term in query:\n",
    "        id_f = idf(term,token_db)\n",
    "        for i in range(len(token_db)):\n",
    "            t_f = tf(term,token_db[i])\n",
    "            ranking_list[i] += tf_idf_weight(tf=t_f, idf=id_f)\n",
    "    return ranking_list\n",
    "\n",
    "def ranking_list(query_list):\n",
    "    '''this function get list of querys and return rank list for each query'''\n",
    "    rank_list = []\n",
    "    for i in range(len(query_list)):\n",
    "        rank_list.append(ranking(query=query_list[i],token_db=token_db))\n",
    "    return rank_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='https://<example>/queries.txt'\n",
    "path='data/queries.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_pageReader(url=None,path=None):\n",
    "    '''this function is for extracting urls\n",
    "    input: url\n",
    "    output: page content'''\n",
    "    \n",
    "    if url is not None and path is None:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(url) as url:\n",
    "            content = str(url.read())   \n",
    "    elif path is not None and url is None:\n",
    "        with open(path) as txt:\n",
    "            content = str(txt.readlines())\n",
    "    return content[1:]\n",
    "  \n",
    "def query_process(text,repl=''):\n",
    "    \"\"\"this function is for processing urls content -\n",
    "    limited in processing base on query file content\"\"\"\n",
    "    import re\n",
    "    pattern = r'\\\\r|\\\\n'   \n",
    "    text = re.sub(pattern,'',text)\n",
    "    text = text.split(sep='.')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(r'\\'','',text[i])\n",
    "        if len(text[i]) == 0:\n",
    "            text.pop(i)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(rank_list,k=10):\n",
    "    '''this function returns most relevant documents\n",
    "    input: rank list\n",
    "    output: top k most relevant list'''\n",
    "    top = {}\n",
    "    rank_dict = rank_list.copy()\n",
    "    for i in range(k):\n",
    "        kmax = max(rank_dict.values())\n",
    "        key = get_key(kmax,rank_dict)\n",
    "        top[key] = kmax\n",
    "        rank_dict.pop(key)\n",
    "    return top\n",
    "\n",
    "def get_key(val,ranks):\n",
    "    '''return dictionary key based on dictionary value\n",
    "    input: value\n",
    "    output: key'''\n",
    "    for key, value in ranks.items():\n",
    "         if val == value:\n",
    "            return key\n",
    "    return \"key doesn't exist\"\n",
    "def showOutput(rank_list):\n",
    "    '''this function is for showing output in pairs\n",
    "    input: rank list\n",
    "    output: list of non-zera relevant documents for each query - (query-ID,docID)'''\n",
    "    lst = []\n",
    "    for i in range(len(ranklist)):\n",
    "        for key,value in ranklist[i].items():\n",
    "            if value > 0:\n",
    "                \n",
    "                lst.append((i + 1, key + 1))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = url_pageReader(path=path)\n",
    "content = query_process(text=content)\n",
    "\n",
    "query_tokens = preProcessor(raw_db=content)\n",
    "ranklist = ranking_list(query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_list = showOutput(ranklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{494: 11.861513096197935,\n",
       " 109: 9.356739524694111,\n",
       " 261: 9.041558523306437,\n",
       " 71: 8.107594890747885,\n",
       " 571: 7.551606003018913,\n",
       " 666: 7.508132796368976,\n",
       " 1355: 7.434533235274976,\n",
       " 1146: 7.306465216264791,\n",
       " 340: 7.182178035058862,\n",
       " 303: 7.1510019503333035}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k(rank_list=ranklist[0], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.I 494\\n.T\\naxisymmetric viscous flow plast very slender bodies\\nof revolution .\\n.A\\nyashura,m.\\n.B\\nj. ae. scs. 29, 1962, 667.\\n.W\\naxisymmetric viscous flow plast very slender bodies\\nof revolution .\\n  axisymmetric viscous flow past unyawed very slender bodies\\nof revolution is treated within the category of the perfect gas .\\nattention is paid especially to the effect of transverse curvature\\nof the body .  from the transformed equations, the similarity\\nconditions are deduced, and the parameter characterizing the\\neffect of transverse curvature is obtained .  several numerical\\nsolutions of similarity equations for hypersonic flows are\\npresented, and upon the basis of these results, the effect of the\\ntransverse-curvature parameter is discussed .  a method of\\napplying the local-similarity approximation to obtain the\\napproximate solution for nonsimilar cases is described, as are\\npractical applications to incompressible flow past a long cylinder and\\nto hypersonic flow past a very slender cone .  comparison with\\nexperimental results shows fair agreement with calculations using\\nthe local-similarity approximation in the present range of\\nexperimental flow conditions .\\n']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_page(page_id=494, db=raw_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
